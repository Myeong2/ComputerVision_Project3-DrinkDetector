{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYEy2dPfGmeHms6SmYn3cl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Myeong2/ComputerVision_Project3-DrinkDetector/blob/master/Untitled17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSIKrOvMYK31"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
        "\n",
        "from collections import deque\n",
        "import random\n",
        "import tensorflow.keras as keras\n",
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, exploration_rate_initial=1.0, exploration_rate_min=0.01,\n",
        "                 exploration_decay=0.995):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95\n",
        "\n",
        "        # 초기 탐험비율, 최소 탐험비율, 탐험비율 감소 시 사용할 값 설정\n",
        "        self.exploration_rate_initial = exploration_rate_initial\n",
        "        self.exploration_rate_min = exploration_rate_min\n",
        "        self.exploration_decay = exploration_decay\n",
        "\n",
        "        # 탐험비율 초기화\n",
        "        self.exploration_rate = self.exploration_rate_initial\n",
        "\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        model = keras.Sequential()\n",
        "        model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=self.state_size))\n",
        "        model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "        model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(keras.layers.Flatten())\n",
        "        model.add(keras.layers.Dense(64, activation='relu'))\n",
        "        model.add(keras.layers.Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, next_state, reward, done):\n",
        "        self.memory.append((state, action, next_state, reward, done))\n",
        "\n",
        "    def get_action(self, state):\n",
        "        # 확률적으로 액션을 선택할지 여부 결정\n",
        "        if np.random.rand() <= self.exploration_rate:\n",
        "            return np.random.randint(self.action_size)\n",
        "        # 모형을 기반으로 액션을 선택\n",
        "        else:\n",
        "            return np.argmax(self.model.predict(state))\n",
        "\n",
        "    def experience_replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, next_state, reward, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                # 타켓값 계산\n",
        "                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
        "            # 학습이 되도록 타겟값 설정\n",
        "            target_f = self.model.predict(state)\n",
        "            target_f[0][action] = target\n",
        "            # 모형 학습\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "\n",
        "        # 탐험비율 감소\n",
        "        if self.exploration_rate > self.exploration_rate_min:\n",
        "            self.exploration_rate *= self.exploration_decay\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)\n",
        "\n",
        "\n",
        "class ImageTransformationEnv(gym.Env):\n",
        "    def __init__(self, init_image=None, classifier=None):\n",
        "        self.action_space = spaces.Discrete(17)\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(64, 64, 3), dtype=np.uint8)\n",
        "\n",
        "        if init_image is not None:\n",
        "            self.init_image = init_image.copy()\n",
        "            self.current_image = init_image.copy()\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_image = self.init_image.copy()\n",
        "        return self.current_image\n",
        "\n",
        "    def _calculate_reward(self, selected_image_normalized):\n",
        "        cat_prob = model.predict(np.expand_dims(selected_image_normalized, axis=0))[0][0]\n",
        "        reward = cat_prob\n",
        "        return reward\n",
        "\n",
        "    def step(self, action):\n",
        "        selected_image_normalized = self.current_image / 255.\n",
        "        reward = self._calculate_reward(selected_image_normalized)\n",
        "\n",
        "        self.current_image = self.apply_action(action)\n",
        "        done = reward >= 0.7  # 타겟 리워드를 0.7로 설정\n",
        "\n",
        "        return self.current_image, reward, done\n",
        "\n",
        "    def render(self):\n",
        "        if self.selected_img is not None:\n",
        "            cv2.imshow(\"Environment Render\", self.selected_img)\n",
        "            cv2.waitKey(1)\n",
        "        else:\n",
        "            print(\"이미지가 없어 화면에 그릴 수 없습니다.\")\n",
        "\n",
        "\n",
        "    # 이미지 변환을 진행하는 코드를 apply_action 메서드로 추가하겠습니다.\n",
        "    def apply_action(self, action):\n",
        "        result_img = self.current_image.copy()\n",
        "\n",
        "        if action == 0:\n",
        "            print(\"상 이동\")\n",
        "            result_img = np.roll(result_img, 1, axis=0)\n",
        "        elif action == 1:\n",
        "            print(\"하 이동\")\n",
        "            result_img = np.roll(result_img, -1, axis=0)\n",
        "        elif action == 2:\n",
        "            print(\"좌 이동\")\n",
        "            result_img = np.roll(result_img, 1, axis=1)\n",
        "        elif action == 3:\n",
        "            print(\"우 이동\")\n",
        "            result_img = np.roll(result_img, -1, axis=1)\n",
        "        elif action == 4:\n",
        "            print(\"R 채널 증가\")\n",
        "            result_img[:, :, 0] = np.clip(result_img[:, :, 0] + 1, 0, 255)\n",
        "        elif action == 5:\n",
        "            print(\"G 채널 증가\")\n",
        "            result_img[:, :, 1] = np.clip(result_img[:, :, 1] + 1, 0, 255)\n",
        "        elif action == 6:\n",
        "            print(\"B 채널 증가\")\n",
        "            result_img[:, :, 2] = np.clip(result_img[:, :, 2] + 1, 0, 255)\n",
        "        elif action == 7:\n",
        "            print(\"R 채널 감소\")\n",
        "            result_img[:, :, 0] = np.clip(result_img[:, :, 0] - 1, 0, 255)\n",
        "        elif action == 8:\n",
        "            print(\"G 채널 감소\")\n",
        "            result_img[:, :, 1] = np.clip(result_img[:, :, 1] - 1, 0, 255)\n",
        "        elif action == 9:\n",
        "            print(\"B 채널 감소\")\n",
        "            result_img[:, :, 2] = np.clip(result_img[:, :, 2] - 1, 0, 255)\n",
        "        elif action == 10:\n",
        "            print(\"가우시안 블러 적용\")\n",
        "            result_img = cv2.GaussianBlur(result_img, (3, 3), 0)\n",
        "        elif action == 11:\n",
        "            print(\"R 채널 10 만큼 증가\")\n",
        "            result_img[:, :, 0] = np.clip(result_img[:, :, 0] + 10, 0, 255)\n",
        "        elif action == 12:\n",
        "            print(\"G 채널 10 만큼 증가\")\n",
        "            result_img[:, :, 1] = np.clip(result_img[:, :, 1] + 10, 0, 255)\n",
        "        elif action == 13:\n",
        "            print(\"B 채널 10 만큼 증가\")\n",
        "            result_img[:, :, 2] = np.clip(result_img[:, :, 2] + 10, 0, 255)\n",
        "        elif action == 14:\n",
        "            print(\"R 채널 10 만큼 감소\")\n",
        "            result_img[:, :, 0] = np.clip(result_img[:, :, 0] - 10, 0, 255)\n",
        "        elif action == 15:\n",
        "            print(\"G 채널 10 만큼 감소\")\n",
        "            result_img[:, :, 1] = np.clip(result_img[:, :, 1] - 10, 0, 255)\n",
        "        elif action == 16:\n",
        "            print(\"B 채널 10 만큼 감소\")\n",
        "            result_img[:, :, 2] = np.clip(result_img[:, :, 2] - 10, 0, 255)\n",
        "\n",
        "        return result_img\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# 이미지 변화 과정을 저장할 폴더 생성\n",
        "if not os.path.exists('result_images'):\n",
        "    os.makedirs('result_images')\n",
        "\n",
        "env = ImageTransformationEnv(init_image=selected_img, classifier=model)\n",
        "state_size = env.observation_space.shape\n",
        "action_size = env.action_space.n\n",
        "agent = DQNAgent(state_size, action_size)\n",
        "\n",
        "episode = 0\n",
        "max_reward = 0\n",
        "\n",
        "while max_reward < 0.9:  # 타겟 리워드를 충족할 때까지 진행\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, *state_size])\n",
        "\n",
        "    done = False\n",
        "    time = 0\n",
        "\n",
        "    episode_images = []\n",
        "\n",
        "    while not done:\n",
        "        action = agent.get_action(state)\n",
        "        next_state, reward, done = env.step(action)\n",
        "\n",
        "        episode_images.append(next_state)  # 이미지 변화 과정 저장\n",
        "        next_state = np.reshape(next_state, [1, *state_size])\n",
        "\n",
        "        agent.remember(state, action, next_state, reward, done)\n",
        "        state = next_state\n",
        "        time += 1\n",
        "\n",
        "        max_reward = max(max_reward, reward)\n",
        "\n",
        "        # 탐험비율 변화\n",
        "        agent.exploration_rate *= agent.exploration_decay\n",
        "        agent.exploration_rate = max(agent.exploration_rate_min, agent.exploration_rate)\n",
        "\n",
        "        if done or time >= 100:  # 프레임 수 제한\n",
        "            break\n",
        "\n",
        "        # 경험 재생\n",
        "        if len(agent.memory) >= batch_size:\n",
        "            agent.experience_replay(batch_size)\n",
        "            if time % 1000 == 0:\n",
        "                agent.save(f'checkpoints/episode_{episode}_time_{time}.h5')\n",
        "\n",
        "    if episode % 10 == 0:\n",
        "        print(f\"에피소드: {episode}, 리워드: {max_reward}, 탐험비율: {agent.exploration_rate}\")\n",
        "\n",
        "    # 변화 과정 이미지 저장\n",
        "    for idx, img in enumerate(episode_images):\n",
        "        cv2.imwrite(f'result_images/episode_{episode}_step_{idx}.png', img)\n",
        "\n",
        "    episode += 1\n"
      ]
    }
  ]
}